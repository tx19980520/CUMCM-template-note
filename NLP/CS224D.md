# CS224D 

### lecture 1

自然语言处理的重要性

传统机器学习是人在制定真正的特征是什么，但是我们DP，我们需要电脑去学习特征。

对于深度学习，我们只是需要向其输入来自世界的原始信号，他是自己来定义特征的，你将得到多层的习得表征。

### lecture 2

one-hot 编码除了第x位，全是0

缺点：

1. 没有表示出任何词汇之间的内在关系概念，每个词语都是独立的

分布相似性：用上下文中的词来表示banking的含义

我们想要的是给每一个词一个密集的向量，让它可以预测目标单词所在文本的其他词汇。

word2vec：不同于独热算法，我们希望计算机能够提取出词的特征，最终得到一个低维的向量来描述一个词语的含义。![vector](vector.png)

两个算法：

1. skip-grams

   在每一个估算步都取一个词作为中心词汇，然后尝试去预测它一定范围内的上下文的词汇，得到一个上下问出现的概率

2. CBOW

   简单将就是完形填空

准备短期之内完成一个word2vec+PCA+SVM的一个小项目

##  lecture 3

Negetive Sampling(负采样)

当我们在进行P(O|C)这个概率的最大化时，相对的我们会把没有关联的词的预测概率降低，这相当于我们要对整个语料库都要进行一次遍历，这是非常耗时的且不划算的，因此我们可以想到我们对于一些高频的词汇进行降低概率就能达到很好的效果，而且多次训练下来也会覆盖到很多的词汇，最终能达到一个很好的效果

奇异值分解（SVD）

首先我们可以通过统计得到共现矩阵(co-occurrence matrix)，然后我们对该矩阵进行奇异值分解，即
$$
X = USV^\top
$$
我们可以以此来进行降维，我们可以抹去S矩阵中的一部分（连带改变UV），在损失一部分信息的情况下仍然能够表示99%的特征，这一步其实不仅仅是在降维，也是在降噪。

但SVD也会有缺点，一旦有新的词汇加入到语料库，将有非常大的一个改变，这将带来非常大的计算量。

我们分为两个方面一个是关于计数的自然语言处理(Count based)，另一部分是关于direct prediction

![compare](compare.png)

#### GloVe模型(Global Vectors)

$$
J(\theta) = \frac{1}{2} \Sigma _{i,j=1}^{W}f(P_{ij})(u_{i}^{T}v_{j} - logP_{ij})^{2}
$$

解释一下这个loss function 其中的P就是我们的共现矩阵，uv两个矩阵还是用来表示词向量的，f函数会尽量的减少对于高频词（高频无实际意义的词）的作用。我们会发现为啥我们需要用uv两个矩阵来表示我们的词，我们实际上在用的时候只用到了一个向量，在训练的时候目标函数的表现会更差，在优化期间有两个相互独立的向量，并且最后把他们整合在一起会更加稳定。

#### 一词多义

tie这个单词可以理解为我们的领带，平局，或者是打结，那么在以单词为单位的词向量空间中，它到底是出现在哪里，这个时候我们自然的会想到，我们应当进一步进行拆分，得到更加基层的向量，因而我们提出了**语境向量**(context vectors)，即每个单词由几个语境向量线性叠加而成，再加上噪音。
$$
v = \Sigma_{i=0}^{D} \alpha_{i}A_{i} + \eta
$$

内在单词向量的评估——词向量类比

例如我们认为男人与女人，类比于国王与女王/王后那么他们的余弦距离应该是一样的

#### 交叉熵与交叉熵误差

首先熵（信息熵）的定义是，一个时间p的发生概率越接近0.5，即其不确定度越高，他的信息熵越大。

交叉熵公式如下：
$$
H(p, q) = -\Sigma_{x}p(x)logq(x)
$$
其中p(x)在机器学习中为样本label，q(x)为模型的预估，分别代表训练样本和模型的分布

最终我们要做到的就是最小化交叉熵误差，即最小化
$$
J(\theta) = \frac{1}{N}\Sigma_{i=1}^{N}-log(\frac{e^{f_{y_{i}}}}{\Sigma_{c=1}^{C}e^{f_{c}}})
$$

 #### Window Classification

简而言之，我们对一个固定大小的窗口进行分类。

我们这个地方得到的loss function 为：
$$
J(\theta) = \frac{1}{N}\Sigma_{i=1}^{N}-log(\frac{e^{f_{y_{i}}}}{\Sigma_{c=1}^{C}e^{f_{c}}})
$$
我们接下来求导

![gradient](gradient.png)

![split_gradient](split_gradient.png)

我们通过上两个图分别得到的是我们窗口的倒数和各个词的倒数





## RNN

首先回顾了之前我们基于统计的传统机器学习的方法，认为这些方法在内存上的要求还是非常高的，并且作出的假设也非常的多，并且制作出来的模型很难在手机平板灯移动端上面进行使用（得到的向量或者矩阵还是非常的大的）

RNN其实就是一直通过迭代的方式，按照单位时间添加矩阵。具体一层的公式计算如下

$x1,…,x_{t−1},x_t,x_{t+1},…,x_T$：表示拥有TT数量词汇的语料中各个词汇对应的词向量。
$h_t=σ(W^{(hh)} h_{t−1}+W^{(hx)} x_t)$：每一轮迭代tt中用于计算隐层输出特征的传递边 
— $x_t∈R^{d}$：在时刻tt的输入词向量。 
— $W^{hx}∈R^{Dh×d}$：利用输入词向量xtxt作为条件计算得到的权重矩阵 
— $W^{hh}∈R^{Dh×Dh}$：利用前一轮迭代的输出作为条件计算得到的权重矩阵 
— $h_{t−1}∈R^{Dh}$：在前一轮迭代t−1t−1中非线性函数的输出结果。并且$h_0∈RDh$为当迭代轮次为t=0t=0时的初始化隐层输出向量。 
— $σ()​$σ()：为非线性分类函数（这里使用sigmoid分类函数）

$y^t=softmax(W(S)ht)$：每一轮迭代t针对全部词汇的输出概率分布。基本上，$y^t$就是当前迭代时，给定文档全部前序文本权值以及观测词向量x(t)x(t)后预测得到的后续词。在这里，$W(S)∈R|V|×DhW(S)∈R^{|V|×Dh}和y∈R^{|V|}$中的变量|V||V|代表全体语料的词典规模。

